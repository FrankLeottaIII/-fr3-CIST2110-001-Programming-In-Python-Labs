{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 - File I/O and Frankenstein\n",
    "### Name:\n",
    "This lab is designed to give you practice with file I/O and to give you a chance to work with a larger program.  You will be writing a program that will read in a text file and then create a new file that is a modified version of the original.  The program will then read in the modified file and print out some statistics about the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in the Frankenstein text using open and read\n",
    "# Open the file\n",
    "with  open('frankenstein.txt', 'r', encoding='utf-8') as f:\n",
    "    # Read the file\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the start of the file (first 1000 characters)\n",
    "# Note: Use slicing to get the first 1000 characters\n",
    "test = text[:1000]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the end of the file (last 1000 characters)\n",
    "# Note: Use slicing to get the last 1000 characters\n",
    "#  H  E  L  L  O\n",
    "#  0  1  2  3  4\n",
    "# -5 -4 -3 -2 -1\n",
    "tail = text[-1000:]\n",
    "print(tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice there is a lot of extra stuff at the beginning and end of the file (The header and footer)\n",
    "# Lets remove the header and footer\n",
    "# Create a method called remove_gutenberg_header_footer that takes in a string and returns a string\n",
    "# The method should remove the header and footer from the string\n",
    "# The method should return the string with the header and footer removed\n",
    "# Note: The header and footer are the same for all Gutenberg texts\n",
    "# Note: The header and footer are marked by the following strings:\n",
    "# *** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***\n",
    "# *** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***\n",
    "\n",
    "\n",
    "\n",
    "def remove_gutenberg_header_footer(text):\n",
    "    start = text.find('*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')\n",
    "    end = text.find('*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')\n",
    "    return text[start:end]\n",
    "# Call the remove_gutenberg_header_footer method with the text and save the result to a variable called text\n",
    "fixed_text = remove_gutenberg_header_footer(text)\n",
    "#printed it out, it worked\n",
    "print(fixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "# Now that we have the text, lets do some analysis\n",
    "# How many chapters are in the text?\n",
    "# Note: Each chapter starts with the text \"Chapter\" and then a number\n",
    "# Note: Use the count method to count the number of chapters but only in the first 1000 characters of the text\n",
    "# Note: There are 24 chapters \n",
    "# Note: Use slicing to get the first 1000 characters\n",
    "print(fixed_text[:1000].count('Chapter'))\n",
    "\n",
    "# Try to remove punctuation\n",
    "# Note: Use the replace method to replace punctuation with nothing\n",
    "# Note: Use the lower method to make the text lowercase\n",
    "# Note: Use the split method to split the text into words\n",
    "# remember methods can be chained together like this: text.replace().replace().replace().lower().split()\n",
    "print(fixed_text.replace('.','').replace(',','').replace('!','').replace('?','').replace(':','').replace(';','').replace('***','').replace('_','').lower().split())\n",
    "\n",
    "# How many unique words are in the text?\n",
    "# A unique word is a word that only appears once in the text  ## I suppose the header counts, as well as numbers, since It technically is a word.\n",
    "# Note: Use the set method to get the unique words\n",
    "# Note: Use the len method to get the number of unique words\n",
    "print(\"--------------------------------------------\")\n",
    "#words = fixed_text.split ---not cleaned properly\n",
    "words_cleaned = fixed_text.replace('.','').replace(',','').replace('!','').replace('?','').replace(':','').replace(';','').replace('***','').replace('/','').replace('_','').replace('-','').replace('--','').lower().split()\n",
    "print(words_cleaned) # note: is - indicate 2 words or 1?  I will just assume 2 word, because its 2 words at the end of the day\n",
    "unique_words_cleaned = set(words_cleaned)\n",
    "print(\"cleaned word lenth: \", len(unique_words_cleaned))\n",
    "# How many times does the word \"monster\" appear in the text?\n",
    "# Note: Use the count method to count the number of times \"monster\" appears in the text\n",
    "print(\"number of times monster appears in the text: \", words_cleaned.count('monster'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get top 10 words in the text\n",
    "# Note: Use the Counter method to get the top 10 words\n",
    "# Note: Use the most_common method to get the top 10 words\n",
    "print(\"--------------------------------------------\")   \n",
    "from collections import Counter\n",
    "word_counts = Counter(words_cleaned)\n",
    "print(word_counts)\n",
    "print(word_counts.most_common(10))\n",
    "\n",
    "# You will need to import Counter from collections to use the Counter method\n",
    "# from collections import Counter \n",
    "# You might need to install collections using pip (in the terminal type: pip install collections)\n",
    "\n",
    "# Print the top 10 words in the text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Extra Credit ####\n",
    "# Show me something interesting about the text that you found using python it can be anything\n",
    "\n",
    "a_counts = \"\"\n",
    "e_counts = \"\"\n",
    "i_counts = \"\"\n",
    "o_counts = \"\"\n",
    "u_counts = \"\"\n",
    "y_counts = \"\"\n",
    "def count_vowels(text)->int:\n",
    "    a_counts = text.count('a')\n",
    "    e_counts = text.count('e')\n",
    "    i_counts = text.count('i')\n",
    "    o_counts = text.count('o')\n",
    "    u_counts = text.count('u')\n",
    "    y_counts = text.count('y')\n",
    "    return int(a_counts), int(e_counts), int(i_counts), o_counts, u_counts, y_counts\n",
    "\n",
    "count_vowels(words_cleaned)\n",
    "\n",
    "\n",
    "\n",
    "if a_counts > e_counts and a_counts > i_counts and a_counts > o_counts and a_counts > u_counts:\n",
    "        print(\"a is the most common vowel\")\n",
    "elif e_counts > a_counts and e_counts > i_counts and e_counts > o_counts and e_counts > u_counts:\n",
    "    print(\"e is the most common vowel\")\n",
    "elif i_counts > a_counts and i_counts > e_counts and i_counts > o_counts and i_counts > u_counts:\n",
    "    print(\"i is the most common vowel\")\n",
    "elif o_counts > a_counts and o_counts > e_counts and o_counts > i_counts and o_counts > u_counts:\n",
    "    print(\"o is the most common vowel\")\n",
    "elif u_counts > a_counts and u_counts > e_counts and u_counts > i_counts and u_counts > o_counts:\n",
    "    print(\"u is the most common vowel\")\n",
    "elif y_counts > a_counts and y_counts > e_counts and y_counts > i_counts and y_counts > o_counts and y_counts > u_counts:\n",
    "    print(\"y is the most common vowel\")\n",
    "\n",
    "\n",
    "#didn't learn about tags or tolkens yet, but this seems to be a good way of getting nouns if i need to count nouns\n",
    "#  #A  1.  I need to import nltk\n",
    "# !pip install nltk\n",
    "# import nltk\n",
    "#     #   2.  I need to download the punkt package from nltk\n",
    "# nltk.download('punkt')\n",
    "#     #   3.  I need to download the averaged_perceptron_tagger package from nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "#     #   4.  I need to tokenize the text\n",
    "# tokens = nltk.word_tokenize(fixed_text)\n",
    "#     #   5.  I need to tag the text\n",
    "# tagged = nltk.pos_tag(tokens)\n",
    "#     #   6.  I need to count the nouns\n",
    "# nouns = [word for word,pos in tagged if pos == 'NN']\n",
    "#     #   7.  I need to print the nouns\n",
    "# print(nouns)\n",
    "#... then count the nouns and print the number of nouns\n",
    "# print(len(nouns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
